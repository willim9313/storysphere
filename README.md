# æ•˜äº‹é ˜åŸŸ - å°èªªè§£æå™¨ / StorySphere - Novel Analyzer

> æ•˜äº‹é ˜åŸŸ(StorySphere)æ˜¯ä¸€å€‹åŸºæ–¼ LLM å’Œå‘é‡è³‡æ–™åº«çš„å°èªªåˆ†æç³»çµ±ï¼Œæä¾›æ–‡æœ¬æ‘˜è¦ã€é—œéµå­—æå–ã€çŸ¥è­˜åœ–è­œæ§‹å»ºå’Œè§’è‰²åˆ†æç­‰åŠŸèƒ½ã€‚

## æ ¸å¿ƒåŠŸèƒ½

### æ–‡ä»¶è™•ç†èˆ‡ç´¢å¼•
- æ”¯æ´ PDF/DOCX æ ¼å¼å°èªªæ–‡ä»¶è¼‰å…¥
- è‡ªå‹•ç« ç¯€è­˜åˆ¥èˆ‡åˆ‡åˆ†ï¼ˆæ”¯æ´ä¸­è‹±æ–‡ç« ç¯€æ¨™é¡Œï¼‰
- æ–‡æœ¬ Chunk åˆ‡åˆ†èˆ‡å‘é‡åŒ–å„²å­˜
- Qdrant å‘é‡è³‡æ–™åº«æ•´åˆ

### è‡ªç„¶èªè¨€è™•ç†
- **æ‘˜è¦ç”Ÿæˆ**ï¼šå±¤ç´šå¼æ‘˜è¦ï¼ˆç« ç¯€ç´šã€å…¨æ›¸ç´šï¼‰
- **é—œéµå­—æå–**ï¼šåŸºæ–¼ MultipartiteRank ç®—æ³•
- **çŸ¥è­˜åœ–è­œ**ï¼šå¯¦é«”é—œä¿‚æŠ½å–èˆ‡æ­£è¦åŒ–
- **è§’è‰²åˆ†æ**ï¼šè§’è‰²å±¬æ€§ã€é—œä¿‚ç¶²çµ¡åˆ†æ

### çŸ¥è­˜åœ–è­œæ§‹å»º
- å¯¦é«”è­˜åˆ¥èˆ‡å±¬æ€§æå–ï¼ˆäººç‰©ã€åœ°é»ã€çµ„ç¹”ç­‰ï¼‰
- é—œä¿‚è­˜åˆ¥èˆ‡ä¸‰å…ƒçµ„æ§‹å»º
- å¯¦é«”æ­£è¦åŒ–èˆ‡å»é‡
- åœ–è­œè¦–è¦ºåŒ–

### åˆ†æåŠŸèƒ½
- è§’è‰²è¡Œç‚ºè»Œè·¡åˆ†æ(in progress)
- ä¸»é¡Œèˆ‡æƒ…æ„Ÿåˆ†æ(in progress)
- å¯«ä½œé¢¨æ ¼åˆ†æ(in progress)
- å±¤ç´šå¼å…§å®¹èšåˆ(in progress)

## å°ˆæ¡ˆæ¶æ§‹

```
storysphere/
â”œâ”€â”€ src/                           # æ ¸å¿ƒç¨‹å¼ç¢¼
â”‚   â”œâ”€â”€ core/                      # æ ¸å¿ƒæ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ indexing/             # å‘é‡è³‡æ–™åº«æ“ä½œ
â”‚   â”‚   â”‚   â””â”€â”€ vector_store.py   # Qdrant å‘é‡å„²å­˜
â”‚   â”‚   â”œâ”€â”€ llm/                  # LLM å®¢æˆ¶ç«¯
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_client.py  # Google Gemini
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_client.py  # Ollama æœ¬åœ°æ¨¡å‹
â”‚   â”‚   â”‚   â””â”€â”€ openai_client.py  # OpenAI API
â”‚   â”‚   â”œâ”€â”€ nlp/                  # NLP å·¥å…·
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_operator.py   # LLM ä»»å‹™å°è£
â”‚   â”‚   â”‚   â””â”€â”€ keyword_extractor.py # é—œéµå­—æå–
â”‚   â”‚   â”œâ”€â”€ kg/                   # çŸ¥è­˜åœ–è­œ
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.py         # è³‡æ–™è¼‰å…¥
â”‚   â”‚   â”‚   â”œâ”€â”€ entity_linker.py  # å¯¦é«”é€£çµ
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_builder.py  # åœ–è­œæ§‹å»º
â”‚   â”‚   â”‚   â””â”€â”€ kg_retriever.py   # è³‡æ–™æª¢ç´¢
â”‚   â”‚   â”œâ”€â”€ validators/           # è³‡æ–™é©—è­‰
â”‚   â”‚   â”‚   â”œâ”€â”€ kg_schema_validator.py    # KG çµæ§‹é©—è­‰
â”‚   â”‚   â”‚   â””â”€â”€ nlp_utils_validator.py    # NLP è¼¸å‡ºé©—è­‰
â”‚   â”‚   â””â”€â”€ utils/                # å·¥å…·å‡½æ•¸
â”‚   â”‚       â”œâ”€â”€ id_generator.py   # ID ç”Ÿæˆå™¨
â”‚   â”‚       â””â”€â”€ output_extractor.py # è¼¸å‡ºè§£æ
â”‚   â”œâ”€â”€ pipelines/                # è³‡æ–™è™•ç†ç®¡é“
â”‚   â”‚   â”œâ”€â”€ preprocessing/        # é è™•ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.py         # æ–‡ä»¶è¼‰å…¥
â”‚   â”‚   â”‚   â”œâ”€â”€ chapter_splitter.py # ç« ç¯€åˆ‡åˆ†
â”‚   â”‚   â”‚   â””â”€â”€ chunk_splitter.py # æ–‡æœ¬åˆ‡åˆ†
â”‚   â”‚   â”œâ”€â”€ feature_extraction/   # ç‰¹å¾µæå–
â”‚   â”‚   â”‚   â””â”€â”€ run_llm_tasks.py  # LLM ä»»å‹™åŸ·è¡Œ
â”‚   â”‚   â”œâ”€â”€ vector_indexing/      # å‘é‡ç´¢å¼•
â”‚   â”‚   â”‚   â””â”€â”€ embed_and_store.py # å‘é‡åŒ–èˆ‡å„²å­˜
â”‚   â”‚   â”œâ”€â”€ nlp/                  # NLP ç®¡é“
â”‚   â”‚   â”‚   â”œâ”€â”€ hierarchical_process.py # å±¤ç´šè™•ç†
â”‚   â”‚   â”‚   â””â”€â”€ keyword_aggregator.py   # é—œéµå­—èšåˆ
â”‚   â”‚   â””â”€â”€ kg/                   # çŸ¥è­˜åœ–è­œç®¡é“
â”‚   â”‚       â”œâ”€â”€ canonical_entity_pipeline.py # å¯¦é«”æ­£è¦åŒ–
â”‚   â”‚       â”œâ”€â”€ graph_construction_pipeline.py # åœ–è­œæ§‹å»º
â”‚   â”‚       â””â”€â”€ entity_attribute_extraction_pipeline.py # å±¬æ€§æå–
â”‚   â””â”€â”€ workflows/                # å·¥ä½œæµç¨‹
â”‚       â”œâ”€â”€ indexing/             # ç´¢å¼•å·¥ä½œæµ
â”‚       â”‚   â””â”€â”€ run_doc_ingestion.py # æ–‡ä»¶æ”å–æµç¨‹
â”‚       â”œâ”€â”€ nlp/                  # NLP å·¥ä½œæµ
â”‚       â”‚   â”œâ”€â”€ generate_hierarchical_summary.py # å±¤ç´šæ‘˜è¦
â”‚       â”‚   â””â”€â”€ generate_hierarchical_keywords.py # å±¤ç´šé—œéµå­—
â”‚       â”œâ”€â”€ kg/                   # çŸ¥è­˜åœ–è­œå·¥ä½œæµ
â”‚       â”‚   â””â”€â”€ run_full_kg_workflow.py # å®Œæ•´ KG æµç¨‹
â”‚       â””â”€â”€ character_analysis/   # è§’è‰²åˆ†æå·¥ä½œæµ
â”‚           â””â”€â”€ character_analysis.py # è§’è‰²åˆ†æ
â”œâ”€â”€ tests/                        # æ¸¬è©¦ç¨‹å¼ç¢¼
â”œâ”€â”€ data/                         # è³‡æ–™ç›®éŒ„
â”‚   â”œâ”€â”€ novella/                 # å°èªªæ–‡ä»¶
â”‚   â”œâ”€â”€ kg_storage/              # çŸ¥è­˜åœ–è­œè³‡æ–™
â”‚   â””â”€â”€ art/                     # åˆ†æçµæœ
â”œâ”€â”€ config/                       # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ schema_kg_story.yaml     # KG çµæ§‹å®šç¾©
â”œâ”€â”€ main_test_*.py               # æ¸¬è©¦è…³æœ¬
â””â”€â”€ requirements.txt             # ä¾è³´å¥—ä»¶
```

## ğŸ› ï¸ å®‰è£èˆ‡è¨­å®š

### ç’°å¢ƒéœ€æ±‚
- Python 3.12.9
- Qdrant å‘é‡è³‡æ–™åº«
- LLM API é‡‘é‘° (Gemini/OpenAI/Ollama)

### å®‰è£æ­¥é©Ÿ

1. **å…‹éš†å°ˆæ¡ˆ**
```bash
git clone <repository-url>
cd storysphere
```

2. **å®‰è£ä¾è³´**
```bash
pip install -r requirements.txt
```

3. **è¨­å®šç’°å¢ƒè®Šæ•¸**
```bash
# è¤‡è£½ç’°å¢ƒè®Šæ•¸æ¨¡æ¿
cp .env.example .env

# ç·¨è¼¯ .env æ–‡ä»¶ï¼Œè¨­å®šä»¥ä¸‹è®Šæ•¸ï¼š
GEMINI_API_KEY=your_gemini_api_key
GEMINI_MODEL=gemini-1.5-flash
QDRANT_HOST=localhost
QDRANT_PORT=6333
```

4. **å•Ÿå‹• Qdrant è³‡æ–™åº«**
```bash
docker run -p 6333:6333 qdrant/qdrant
```

## ä½¿ç”¨æ–¹å¼

### 1. æ–‡ä»¶æ”å–èˆ‡è™•ç†

```python
from src.workflows.indexing.run_doc_ingestion import run_ingestion_pipeline

# è™•ç†å°èªªæ–‡ä»¶ï¼Œç”Ÿæˆå‘é‡ç´¢å¼•
run_ingestion_pipeline(
    input_dir="./data/novella",
    collection_name="MyNovel",
    api_key="your_api_key",
    model_name="gemini-1.5-flash",
    limit_pages=10  # é™åˆ¶è™•ç†é æ•¸ï¼ˆå¯é¸ï¼‰
)
```

### 2. å±¤ç´šæ‘˜è¦ç”Ÿæˆ

```python
from src.workflows.nlp.generate_hierarchical_summary import (
    gen_hierarchical_chapter_summary,
    gen_hierarchical_book_summary
)

# ç”Ÿæˆç« ç¯€ç´šæ‘˜è¦
gen_hierarchical_chapter_summary(
    target_collection_doc_id="doc_uuid",
    source_collection="MyNovel"
)

# ç”Ÿæˆå…¨æ›¸æ‘˜è¦
gen_hierarchical_book_summary(
    target_collection_doc_id="doc_uuid",
    omni_chapters_collection="chapter_summaries",
    omni_books_collection="book_summaries"
)
```

### 3. çŸ¥è­˜åœ–è­œæ§‹å»º

```python
from src.workflows.kg.run_full_kg_workflow import run_full_kg_workflow

# åŸ·è¡Œå®Œæ•´çš„çŸ¥è­˜åœ–è­œæ§‹å»ºæµç¨‹
run_full_kg_workflow(
    entity_path="./data/kg_storage/kg_entity_set.json",
    relation_path="./data/kg_storage/kg_relation_set.json"
)
```

### 4. è§’è‰²åˆ†æ

```python
from src.workflows.character_analysis.character_analysis import (
    run_character_analysis_workflow
)

# åˆ†ææŒ‡å®šè§’è‰²
run_character_analysis_workflow(
    target_role=["Harry Potter", "Hermione Granger"],
    kg_entity_path="./data/kg_storage/kg_entity_set.json"
)
```

## ğŸ“Š æ ¸å¿ƒæ¨¡çµ„èªªæ˜

### Vector Store (å‘é‡å„²å­˜)
- **ä½ç½®**: `src/core/indexing/vector_store.py`
- **åŠŸèƒ½**: Qdrant å‘é‡è³‡æ–™åº«æ“ä½œå°è£
- **ä¸»è¦æ–¹æ³•**: `store_chunk()`, `search()`, `get_by_id()`

### LLM Operator (LLM æ“ä½œå™¨)
- **ä½ç½®**: `src/core/nlp/llm_operator.py`
- **åŠŸèƒ½**: çµ±ä¸€çš„ LLM ä»»å‹™ä»‹é¢
- **æ”¯æ´ä»»å‹™**: æ‘˜è¦ã€é—œéµå­—æå–ã€çŸ¥è­˜åœ–è­œæŠ½å–

### Knowledge Graph Pipeline (çŸ¥è­˜åœ–è­œç®¡é“)
- **ä½ç½®**: `src/pipelines/kg/`
- **åŠŸèƒ½**: å¯¦é«”æŠ½å–ã€æ­£è¦åŒ–ã€åœ–è­œæ§‹å»º
- **è¼¸å‡º**: JSON æ ¼å¼çš„å¯¦é«”é—œä¿‚è³‡æ–™

### Hierarchical Processor (å±¤ç´šè™•ç†å™¨)
- **ä½ç½®**: `src/pipelines/nlp/hierarchical_process.py`
- **åŠŸèƒ½**: å¤šå±¤ç´šå…§å®¹èšåˆï¼ˆchunk â†’ ç« ç¯€ â†’ å…¨æ›¸ï¼‰

## ğŸ”§ æ¸¬è©¦è…³æœ¬

å°ˆæ¡ˆæä¾›å¤šå€‹æ¸¬è©¦è…³æœ¬ä¾›å¿«é€Ÿé©—è­‰åŠŸèƒ½ï¼š

- `main_test_run_doc_ingestion.py`: æ¸¬è©¦æ–‡ä»¶æ”å–æµç¨‹
- `main_test_kg.py`: æ¸¬è©¦çŸ¥è­˜åœ–è­œæ§‹å»º
- `main_test_hierarchical.py`: æ¸¬è©¦å±¤ç´šæ‘˜è¦èˆ‡é—œéµå­—
- `main_test_entity_retriever.py`: æ¸¬è©¦å¯¦é«”æª¢ç´¢èˆ‡è§’è‰²åˆ†æ

## ğŸ“ˆ æ”¯æ´çš„æ–‡ä»¶æ ¼å¼

- **PDF**: ä½¿ç”¨ PyPDF2/pypdf è§£æ
- **DOCX**: ä½¿ç”¨ python-docx è™•ç†
- **ç« ç¯€è­˜åˆ¥**: æ”¯æ´ä¸­è‹±æ–‡ç« ç¯€æ¨™é¡Œæ¨¡å¼

## ğŸ¯ æ‡‰ç”¨å ´æ™¯

- ğŸ“š **æ–‡å­¸ç ”ç©¶**: è‡ªå‹•åŒ–æ–‡æœ¬åˆ†æèˆ‡æ‘˜è¦
- ğŸ­ **è§’è‰²ç ”ç©¶**: è§’è‰²é—œä¿‚ç¶²çµ¡èˆ‡è¡Œç‚ºåˆ†æ
- ğŸ“– **å…§å®¹æ•´ç†**: å¤§éƒ¨é ­å°èªªçš„çµæ§‹åŒ–æ•´ç†
- ğŸ” **èªç¾©æœå°‹**: åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦çš„å…§å®¹æª¢ç´¢

## âš ï¸ æ³¨æ„äº‹é …

1. **API é…é¡**: LLM API å‘¼å«æœƒç”¢ç”Ÿè²»ç”¨ï¼Œè«‹æ³¨æ„ä½¿ç”¨é‡
2. **è¨˜æ†¶é«”ä½¿ç”¨**: å¤§å‹æ–‡ä»¶è™•ç†éœ€è¦å……è¶³è¨˜æ†¶é«”
3. **å‘é‡è³‡æ–™åº«**: ç¢ºä¿ Qdrant æœå‹™æ­£å¸¸é‹è¡Œ
4. **æ¨¡å‹é¸æ“‡**: ä¸åŒ LLM æ¨¡å‹æ•ˆæœå¯èƒ½æœ‰å·®ç•°

## ğŸ¤ è²¢ç»æŒ‡å—

åˆ¥äº†ï¼Œæˆ‘é‚„æœ‰å¾ˆå¤šæƒ³å¯¦ç¾çš„åŠŸèƒ½ã€‚  
ä»¥ä¸‹åˆ—èˆ‰å¹¾å€‹æœ‰é»æ„æ€çš„ï¼š
- å¢åŠ çŸ¥è­˜åœ–è­œä¸­é—œä¿‚é æ¸¬çš„æ¨è«–åŠŸèƒ½
- è§’è‰²åŠ‡æƒ…æ¨¡æ“¬ï¼šç”¨æˆ¶è‡ªå·±æ±ºå®šå¯èƒ½çš„æƒ…å¢ƒï¼Œåˆ©ç”¨æ—¢æœ‰è§’è‰²è³‡è¨Šåšå‡ºwhat if scenario
- åœ–åƒç”Ÿæˆï¼šusing text2image model
    - å¯ä»¥æŒ‡å®šé¢¨æ ¼å°è§’è‰²ç”Ÿæˆåœ–åƒ
    - å°åŠ‡æƒ…æˆ–æ˜¯ç‰¹å®šæ®µè½ç”Ÿæˆåœ–åƒç­‰
    - ä½†é€™å€‹å‘å¯èƒ½æœ‰é»å¤§
- æä¾›æ›´å¤šè§£æä¸Šçš„å·¥å…·ä»¥åŠèªªæ˜ï¼Œè®“ä½¿ç”¨è€…æ›´æ˜ç™½è‡ªå·±çœ‹çš„æ±è¥¿æ˜¯ä»€éº¼


---

# StorySphere - Novel Analyzer

> An intelligent novel analysis system based on LLM and vector databases, providing text summarization, keyword extraction, knowledge graph construction, and character analysis features.

## Core Features

### Document Processing & Indexing
- Support for PDF/DOCX novel file loading
- Automatic chapter identification and segmentation (supports Chinese and English chapter titles)
- Text chunk splitting and vector storage
- Qdrant vector database integration

### Natural Language Processing
- **Summary Generation**: Hierarchical summaries (chapter-level, book-level)
- **Keyword Extraction**: Based on MultipartiteRank algorithm
- **Knowledge Graph**: Entity relationship extraction and normalization
- **Character Analysis**: Character attributes and relationship network analysis

### Knowledge Graph Construction
- Entity identification and attribute extraction (characters, locations, organizations, etc.)
- Relationship identification and triplet construction
- Entity normalization and deduplication
- Graph visualization

### Analysis Features
- Character behavior trajectory analysis (in progress)
- Theme and sentiment analysis (in progress)
- Writing style analysis (in progress)
- Hierarchical content aggregation (in progress)

## Project Architecture

```
storysphere/
â”œâ”€â”€ src/                           # Core source code
â”‚   â”œâ”€â”€ core/                      # Core modules
â”‚   â”‚   â”œâ”€â”€ indexing/             # Vector database operations
â”‚   â”‚   â”‚   â””â”€â”€ vector_store.py   # Qdrant vector storage
â”‚   â”‚   â”œâ”€â”€ llm/                  # LLM clients
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_client.py  # Google Gemini
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_client.py  # Ollama local models
â”‚   â”‚   â”‚   â””â”€â”€ openai_client.py  # OpenAI API
â”‚   â”‚   â”œâ”€â”€ nlp/                  # NLP tools
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_operator.py   # LLM task wrapper
â”‚   â”‚   â”‚   â””â”€â”€ keyword_extractor.py # Keyword extraction
â”‚   â”‚   â”œâ”€â”€ kg/                   # Knowledge graph
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.py         # Data loading
â”‚   â”‚   â”‚   â”œâ”€â”€ entity_linker.py  # Entity linking
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_builder.py  # Graph construction
â”‚   â”‚   â”‚   â””â”€â”€ kg_retriever.py   # Data retrieval
â”‚   â”‚   â”œâ”€â”€ validators/           # Data validation
â”‚   â”‚   â”‚   â”œâ”€â”€ kg_schema_validator.py    # KG structure validation
â”‚   â”‚   â”‚   â””â”€â”€ nlp_utils_validator.py    # NLP output validation
â”‚   â”‚   â””â”€â”€ utils/                # Utility functions
â”‚   â”‚       â”œâ”€â”€ id_generator.py   # ID generator
â”‚   â”‚       â””â”€â”€ output_extractor.py # Output parsing
â”‚   â”œâ”€â”€ pipelines/                # Data processing pipelines
â”‚   â”‚   â”œâ”€â”€ preprocessing/        # Preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.py         # Document loading
â”‚   â”‚   â”‚   â”œâ”€â”€ chapter_splitter.py # Chapter splitting
â”‚   â”‚   â”‚   â””â”€â”€ chunk_splitter.py # Text chunking
â”‚   â”‚   â”œâ”€â”€ feature_extraction/   # Feature extraction
â”‚   â”‚   â”‚   â””â”€â”€ run_llm_tasks.py  # LLM task execution
â”‚   â”‚   â”œâ”€â”€ vector_indexing/      # Vector indexing
â”‚   â”‚   â”‚   â””â”€â”€ embed_and_store.py # Vectorization & storage
â”‚   â”‚   â”œâ”€â”€ nlp/                  # NLP pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ hierarchical_process.py # Hierarchical processing
â”‚   â”‚   â”‚   â””â”€â”€ keyword_aggregator.py   # Keyword aggregation
â”‚   â”‚   â””â”€â”€ kg/                   # Knowledge graph pipeline
â”‚   â”‚       â”œâ”€â”€ canonical_entity_pipeline.py # Entity normalization
â”‚   â”‚       â”œâ”€â”€ graph_construction_pipeline.py # Graph construction
â”‚   â”‚       â””â”€â”€ entity_attribute_extraction_pipeline.py # Attribute extraction
â”‚   â””â”€â”€ workflows/                # Workflows
â”‚       â”œâ”€â”€ indexing/             # Indexing workflows
â”‚       â”‚   â””â”€â”€ run_doc_ingestion.py # Document ingestion flow
â”‚       â”œâ”€â”€ nlp/                  # NLP workflows
â”‚       â”‚   â”œâ”€â”€ generate_hierarchical_summary.py # Hierarchical summary
â”‚       â”‚   â””â”€â”€ generate_hierarchical_keywords.py # Hierarchical keywords
â”‚       â”œâ”€â”€ kg/                   # Knowledge graph workflows
â”‚       â”‚   â””â”€â”€ run_full_kg_workflow.py # Full KG workflow
â”‚       â””â”€â”€ character_analysis/   # Character analysis workflows
â”‚           â””â”€â”€ character_analysis.py # Character analysis
â”œâ”€â”€ tests/                        # Test code
â”œâ”€â”€ data/                         # Data directory
â”‚   â”œâ”€â”€ novella/                 # Novel files
â”‚   â”œâ”€â”€ kg_storage/              # Knowledge graph data
â”‚   â””â”€â”€ art/                     # Analysis results
â”œâ”€â”€ config/                       # Configuration files
â”‚   â””â”€â”€ schema_kg_story.yaml     # KG structure definition
â”œâ”€â”€ main_test_*.py               # Test scripts
â””â”€â”€ requirements.txt             # Dependencies
```

## ğŸ› ï¸ Installation & Setup

### Requirements
- Python 3.12.9
- Qdrant vector database
- LLM API keys (Gemini/OpenAI/Ollama)

### Installation Steps

1. **Clone the project**
```bash
git clone <repository-url>
cd storysphere
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure environment variables**
```bash
# Copy environment template
cp .env.example .env

# Edit .env file and set the following variables:
GEMINI_API_KEY=your_gemini_api_key
GEMINI_MODEL=gemini-1.5-flash
QDRANT_HOST=localhost
QDRANT_PORT=6333
```

4. **Start Qdrant database**
```bash
docker run -p 6333:6333 qdrant/qdrant
```

## Usage

### 1. Document Ingestion & Processing

```python
from src.workflows.indexing.run_doc_ingestion import run_ingestion_pipeline

# Process novel files and generate vector index
run_ingestion_pipeline(
    input_dir="./data/novella",
    collection_name="MyNovel",
    api_key="your_api_key",
    model_name="gemini-1.5-flash",
    limit_pages=10  # Limit processing pages (optional)
)
```

### 2. Hierarchical Summary Generation

```python
from src.workflows.nlp.generate_hierarchical_summary import (
    gen_hierarchical_chapter_summary,
    gen_hierarchical_book_summary
)

# Generate chapter-level summaries
gen_hierarchical_chapter_summary(
    target_collection_doc_id="doc_uuid",
    source_collection="MyNovel"
)

# Generate book-level summary
gen_hierarchical_book_summary(
    target_collection_doc_id="doc_uuid",
    omni_chapters_collection="chapter_summaries",
    omni_books_collection="book_summaries"
)
```

### 3. Knowledge Graph Construction

```python
from src.workflows.kg.run_full_kg_workflow import run_full_kg_workflow

# Execute complete knowledge graph construction workflow
run_full_kg_workflow(
    entity_path="./data/kg_storage/kg_entity_set.json",
    relation_path="./data/kg_storage/kg_relation_set.json"
)
```

### 4. Character Analysis

```python
from src.workflows.character_analysis.character_analysis import (
    run_character_analysis_workflow
)

# Analyze specific characters
run_character_analysis_workflow(
    target_role=["Harry Potter", "Hermione Granger"],
    kg_entity_path="./data/kg_storage/kg_entity_set.json"
)
```

## ğŸ“Š Core Module Descriptions

### Vector Store
- **Location**: `src/core/indexing/vector_store.py`
- **Function**: Qdrant vector database operations wrapper
- **Main Methods**: `store_chunk()`, `search()`, `get_by_id()`

### LLM Operator
- **Location**: `src/core/nlp/llm_operator.py`
- **Function**: Unified LLM task interface
- **Supported Tasks**: Summarization, keyword extraction, knowledge graph extraction

### Knowledge Graph Pipeline
- **Location**: `src/pipelines/kg/`
- **Function**: Entity extraction, normalization, graph construction
- **Output**: JSON format entity relationship data

### Hierarchical Processor
- **Location**: `src/pipelines/nlp/hierarchical_process.py`
- **Function**: Multi-level content aggregation (chunk â†’ chapter â†’ book)

## ğŸ”§ Test Scripts

The project provides multiple test scripts for quick functionality verification:

- `main_test_run_doc_ingestion.py`: Test document ingestion workflow
- `main_test_kg.py`: Test knowledge graph construction
- `main_test_hierarchical.py`: Test hierarchical summary & keywords
- `main_test_entity_retriever.py`: Test entity retrieval & character analysis

## ğŸ“ˆ Supported File Formats

- **PDF**: Parsed using PyPDF2/pypdf
- **DOCX**: Processed using python-docx
- **Chapter Recognition**: Supports Chinese and English chapter title patterns

## ğŸ¯ Use Cases

- ğŸ“š **Literary Research**: Automated text analysis and summarization
- ğŸ­ **Character Studies**: Character relationship networks and behavior analysis
- ğŸ“– **Content Organization**: Structured organization of large novels
- ğŸ” **Semantic Search**: Vector similarity-based content retrieval

## âš ï¸ Notes

1. **API Quotas**: LLM API calls incur costs, please monitor usage
2. **Memory Usage**: Large document processing requires sufficient memory
3. **Vector Database**: Ensure Qdrant service is running properly
4. **Model Selection**: Different LLM models may have varying performance

## ğŸ¤ Contributing

There are still many features I'd like to implement.  
Here are some interesting ones:

- Add inference capabilities for relationship prediction in knowledge graphs
- Character story simulation: Users can decide possible scenarios and use existing character information for "what if" scenarios
- Image generation: using text2image models
    - Generate character images with specified styles
    - Generate images for plots or specific passages
    - But this might be a big undertaking
- Provide more analysis tools and explanations to help users better understand what they're looking at

---

> ğŸ’¡ **Tip**: This is a backend analysis framework, suitable for developers who need deep text analysis capabilities. If you need a frontend interface, please develop or integrate existing UI frameworks yourself.
